{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d937a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36fdfff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.14.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>=1.6.1 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.8.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/miniconda3/envs/adnlp_hw2_env/lib/python3.13/site-packages (from beautifulsoup4->bs4) (4.15.0)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
      "Downloading soupsieve-2.8.3-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [bs4]\n",
      "\u001b[1A\u001b[2KSuccessfully installed beautifulsoup4-4.14.3 bs4-0.0.2 soupsieve-2.8.3\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d59bf751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51 HTML files in: baseline_data\n",
      "Converted 51 files -> baseline_data/wikipedia_md_cleaned\n",
      "Example output: baseline_data/wikipedia_md_cleaned/Andrew Carnegie - Wikipedia.md\n"
     ]
    }
   ],
   "source": [
    "# Notebook cell: Convert all Wikipedia .htm/.html files in a folder to cleaned Markdown files\n",
    "# (Improved: drop tail sections, remove link titles, extra wiki junk removal)\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import re\n",
    "\n",
    "# ---- 0) Config: set your input folder here ----\n",
    "INPUT_DIR = Path(\"baseline_data\")\n",
    "OUTPUT_DIR = INPUT_DIR / \"wikipedia_md_cleaned\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- 1) Dependencies (install if missing) ----\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except ImportError:\n",
    "    !pip -q install beautifulsoup4 lxml\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "try:\n",
    "    from markdownify import markdownify as md\n",
    "except ImportError:\n",
    "    !pip -q install markdownify\n",
    "    from markdownify import markdownify as md\n",
    "\n",
    "\n",
    "# ---- 2) Wikipedia-specific cleaning helpers ----\n",
    "WIKI_REMOVE_SELECTORS = [\n",
    "    # navigation / chrome\n",
    "    \"header\", \"footer\", \"nav\",\n",
    "    \"#mw-head\", \"#mw-panel\", \"#siteSub\", \"#contentSub\",\n",
    "    \".mw-editsection\", \".mw-editsection-bracket\",\n",
    "    \"#toc\", \".toc\", \".vector-toc\",\n",
    "    \".mw-jump-link\", \".mw-portlet\", \".vector-column-start\",\n",
    "    \".vector-page-toolbar\", \".vector-page-titlebar\",\n",
    "    \".noprint\", \".mw-footer\", \".printfooter\",\n",
    "    # references / citations / hatnotes\n",
    "    \"sup.reference\", \".reference\", \".reflist\", \"ol.references\",\n",
    "    \".hatnote\", \".dablink\", \".shortdescription\", \".ambox\",\n",
    "    # infobox + navboxes + sidebars\n",
    "    \"table.infobox\", \"table.vertical-navbox\", \"table.navbox\",\n",
    "    \".navbox\", \".vertical-navbox\", \".sidebar\",\n",
    "    # media / images\n",
    "    \"figure\", \"img\", \".thumb\", \".gallery\", \".mw-file-element\",\n",
    "    # math/code\n",
    "    \"math\", \"code\", \"pre\",\n",
    "]\n",
    "\n",
    "DROP_TAIL_SECTIONS = {\n",
    "    \"See also\",\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"Further reading\",\n",
    "    \"Notes\",\n",
    "}\n",
    "\n",
    "def _squeeze_blank_lines(s: str) -> str:\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def _drop_tail_sections_markdown(md_text: str) -> str:\n",
    "    \"\"\"Drop low-value sections by stopping at the first matching heading.\"\"\"\n",
    "    out = []\n",
    "    for line in md_text.splitlines():\n",
    "        m = re.match(r\"^(#{2,6})\\s+(.*)\\s*$\", line.strip())\n",
    "        if m:\n",
    "            heading_text = m.group(2).strip()\n",
    "            if heading_text in DROP_TAIL_SECTIONS:\n",
    "                break\n",
    "        out.append(line)\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def _post_clean_markdown(md_text: str) -> str:\n",
    "    # Remove link titles: [text](url \"title\") -> [text](url)\n",
    "    md_text = re.sub(r'(\\[[^\\]]+\\]\\([^\\s\\)]+)\\s+\"[^\"]*\"\\)', r\"\\1)\", md_text)\n",
    "\n",
    "    # Remove leftover citation markers like [1], [23]\n",
    "    md_text = re.sub(r\"\\[\\d+\\]\", \"\", md_text)\n",
    "\n",
    "    # Remove empty link artifacts like []()\n",
    "    md_text = re.sub(r\"\\[\\s*\\]\\([^)]+\\)\", \"\", md_text)\n",
    "\n",
    "    # Remove common Wikimedia Commons template sentence if it appears\n",
    "    md_text = re.sub(r\"^Wikimedia Commons has media related to .*?\\.\\s*$\", \"\", md_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Drop tail sections\n",
    "    md_text = _drop_tail_sections_markdown(md_text)\n",
    "\n",
    "    return _squeeze_blank_lines(md_text)\n",
    "\n",
    "def extract_wikipedia_main_html(html: str) -> tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Returns: (title, source_url, main_html)\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Title\n",
    "    title = soup.title.get_text(strip=True) if soup.title else \"Wikipedia Page\"\n",
    "\n",
    "    # Canonical URL if available\n",
    "    canonical = soup.find(\"link\", rel=\"canonical\")\n",
    "    source_url = canonical[\"href\"].strip() if canonical and canonical.get(\"href\") else \"\"\n",
    "\n",
    "    # Main content container (Wikipedia)\n",
    "    main = soup.select_one(\"#mw-content-text\") or soup.select_one(\"main#content\") or soup.select_one(\"#content\")\n",
    "    if not main:\n",
    "        main = soup.body or soup\n",
    "\n",
    "    # Remove unwanted elements\n",
    "    for sel in WIKI_REMOVE_SELECTORS:\n",
    "        for tag in main.select(sel):\n",
    "            tag.decompose()\n",
    "\n",
    "    # Remove edit links or residual UI spans if any\n",
    "    for tag in main.select(\"span.mw-editsection\"):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Remove \"mw-cite-backlink\" / citation back-links if present\n",
    "    for tag in main.select(\".mw-cite-backlink\"):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Convert relative wiki links to absolute\n",
    "    for a in main.select(\"a[href]\"):\n",
    "        href = a.get(\"href\", \"\")\n",
    "        if href.startswith(\"/wiki/\"):\n",
    "            a[\"href\"] = \"https://en.wikipedia.org\" + href\n",
    "        elif href.startswith(\"//\"):\n",
    "            a[\"href\"] = \"https:\" + href\n",
    "\n",
    "    # Some pages keep \"Coordinates\" in a small span; remove if still present\n",
    "    for tag in main.find_all(string=re.compile(r\"Coordinates\", re.IGNORECASE)):\n",
    "        if tag.parent and tag.parent.name in {\"span\", \"small\"}:\n",
    "            tag.parent.decompose()\n",
    "\n",
    "    # Remove tables that sometimes survive (e.g., metadata, authority control)\n",
    "    for tag in main.select(\"table.metadata, table.ambox\"):\n",
    "        tag.decompose()\n",
    "\n",
    "    main_html = str(main)\n",
    "    return title, source_url, main_html\n",
    "\n",
    "def wikipedia_html_to_clean_markdown(html: str) -> tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Returns: (title, source_url, markdown_text)\n",
    "    \"\"\"\n",
    "    title, source_url, main_html = extract_wikipedia_main_html(html)\n",
    "\n",
    "    # Convert to Markdown\n",
    "    md_text = md(main_html, heading_style=\"ATX\", bullets=\"-\")\n",
    "\n",
    "    # Post-clean in Markdown space\n",
    "    md_text = _post_clean_markdown(md_text)\n",
    "\n",
    "    return title, source_url, md_text\n",
    "\n",
    "\n",
    "# ---- 3) Batch convert ----\n",
    "html_files = sorted(list(INPUT_DIR.glob(\"*.htm\")) + list(INPUT_DIR.glob(\"*.html\")))\n",
    "print(f\"Found {len(html_files)} HTML files in: {INPUT_DIR}\")\n",
    "\n",
    "now_iso = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "converted = 0\n",
    "for fp in html_files:\n",
    "    html = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    title, source_url, md_text = wikipedia_html_to_clean_markdown(html)\n",
    "\n",
    "    # YAML front matter (matches your earlier style)\n",
    "    front_matter = [\n",
    "        \"---\",\n",
    "        f\"source_url: {source_url or ''}\",\n",
    "        f\"scraped_at: {now_iso}\",\n",
    "        f\"title: {title}\",\n",
    "        \"description: \",\n",
    "        \"---\",\n",
    "        \"\",\n",
    "    ]\n",
    "    out_text = \"\\n\".join(front_matter) + md_text + \"\\n\"\n",
    "\n",
    "    out_name = fp.stem + \".md\"\n",
    "    out_path = OUTPUT_DIR / out_name\n",
    "    out_path.write_text(out_text, encoding=\"utf-8\")\n",
    "\n",
    "    converted += 1\n",
    "\n",
    "print(f\"Converted {converted} files -> {OUTPUT_DIR}\")\n",
    "print(\"Example output:\", (OUTPUT_DIR / (html_files[0].stem + \".md\")) if html_files else \"N/A\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adnlp_hw2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
