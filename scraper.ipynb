{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e788e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q requests python-dotenv pymupdf4llm lxml beautifulsoup4 markdownify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52260652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API Key 已加载：fc-5da20...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "fc_api_key = os.getenv(\"FIRECRAWL_API_KEY\") or os.getenv(\"FC_API_KEY\")\n",
    "if not fc_api_key:\n",
    "    raise RuntimeError(\"Set FIRECRAWL_API_KEY or FC_API_KEY in env/.env\")\n",
    "print(\"API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e95cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "sys.path.insert(0, str(ROOT / \"scripts\"))\n",
    "sys.path.insert(0, str(ROOT / \"scripts/web_scrapers\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61746375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.web_scrapers.scrape_wikipedia import scrape_all as run_wiki, TARGETS as WIKI_TARGETS\n",
    "from scripts.web_scrapers.scrape_britannica import scrape_all as run_brit, TARGETS as BRIT_TARGETS\n",
    "from scripts.web_scrapers.scrape_visitpittsburgh import scrape_all as run_visit, TARGETS as VISIT_TARGETS\n",
    "from scripts.web_scrapers.scrape_cmu import scrape_all as run_cmu, TARGETS as CMU_TARGETS\n",
    "\n",
    "print(\"Running Collection A web scrapers...\")\n",
    "wiki_results = run_wiki(WIKI_TARGETS, api_key=fc_api_key)\n",
    "brit_results = run_brit(BRIT_TARGETS, api_key=fc_api_key)\n",
    "visit_results = run_visit(VISIT_TARGETS, api_key=fc_api_key)\n",
    "cmu_results = run_cmu(CMU_TARGETS, api_key=fc_api_key)\n",
    "print(\"Collection A done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd686f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.web_scrapers.scrape_pdf_collection_b import process_all as run_pdf, TARGETS as PDF_TARGETS\n",
    "\n",
    "print(\"Running Collection B PDFs...\")\n",
    "pdf_results = run_pdf(PDF_TARGETS)\n",
    "print(\"Collection B done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca85397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.web_scrapers.scrape_events_collection_c import scrape_all as run_events, TARGETS as C_TARGETS\n",
    "from scripts.web_scrapers.process_recurring_events_csv_c import process_csv as process_recurring\n",
    "\n",
    "print(\"Running Collection C events...\")\n",
    "events_results = run_events(C_TARGETS, api_key=fc_api_key, delay=2.0, skip_existing=False)\n",
    "csv_path = Path(\"data/raw/C/recurring_events.csv\")\n",
    "recurring_results = None\n",
    "if csv_path.exists():\n",
    "    recurring_results = process_recurring(csv_path, source_url=\"https://www.pittsburghmagazine.com/\")\n",
    "else:\n",
    "    print(\"recurring_events.csv missing\")\n",
    "print(\"Collection C done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5fe175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import runpy\n",
    "import scripts.web_scrapers.crawl_collection_d as crawl_d\n",
    "\n",
    "print(\"Running Collection D crawl...\")\n",
    "crawl_d.API_KEY = fc_api_key\n",
    "crawl_d.HEADERS = {\"Authorization\": f\"Bearer {fc_api_key}\", \"Content-Type\": \"application/json\"}\n",
    "crawl_results = crawl_d.crawl_all(crawl_d.CRAWL_TASKS, dry_run=False)\n",
    "\n",
    "restaurants_csv = ROOT / \"data/raw/D/visitpittsburgh_restaurants.csv\"\n",
    "if restaurants_csv.exists():\n",
    "    runpy.run_path(str(ROOT / \"scripts/web_scrapers/scrape_restaurants_d.py\"), run_name=\"__main__\")\n",
    "else:\n",
    "    print(\"restaurants CSV missing\")\n",
    "print(\"Collection D done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f46d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.web_scrapers.cleaner_d import clean_all\n",
    "\n",
    "print(\"Cleaning Collection D markdown...\")\n",
    "clean_stats = clean_all(strip_button_lines=True, dry_run=False)\n",
    "print(\"Clean done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a8bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "INPUT_DIR = Path(\"baseline_data\")\n",
    "OUTPUT_DIR = INPUT_DIR / \"wikipedia_md_cleaned\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REMOVE_SELECTORS = [\n",
    "    \"header\", \"footer\", \"nav\",\n",
    "    \"#mw-head\", \"#mw-panel\", \"#siteSub\", \"#contentSub\",\n",
    "    \".mw-editsection\", \".mw-editsection-bracket\",\n",
    "    \"#toc\", \".toc\", \".vector-toc\",\n",
    "    \".mw-jump-link\", \".mw-portlet\", \".vector-column-start\",\n",
    "    \".vector-page-toolbar\", \".vector-page-titlebar\",\n",
    "    \".noprint\", \".mw-footer\", \".printfooter\",\n",
    "    \"sup.reference\", \".reference\", \".reflist\", \"ol.references\",\n",
    "    \".hatnote\", \".dablink\", \".shortdescription\", \".ambox\",\n",
    "    \"table.infobox\", \"table.vertical-navbox\", \"table.navbox\",\n",
    "    \".navbox\", \".vertical-navbox\", \".sidebar\",\n",
    "    \"figure\", \"img\", \".thumb\", \".gallery\", \".mw-file-element\",\n",
    "    \"math\", \"code\", \"pre\",\n",
    "]\n",
    "DROP_TAIL = {\"See also\", \"References\", \"External links\", \"Further reading\", \"Notes\"}\n",
    "\n",
    "def _squeeze_blank(s: str) -> str:\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def _drop_tail(md_text: str) -> str:\n",
    "    out = []\n",
    "    for line in md_text.splitlines():\n",
    "        m = re.match(r\"^(#{2,6})\\s+(.*)\\s*$\", line.strip())\n",
    "        if m and m.group(2).strip() in DROP_TAIL:\n",
    "            break\n",
    "        out.append(line)\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def _post_clean(md_text: str) -> str:\n",
    "    md_text = re.sub(r'(\\[[^\\]]+\\]\\([^\\s\\)]+)\\s+\"[^\"]*\"\\)', r\"\\1)\", md_text)\n",
    "    md_text = re.sub(r\"\\[\\d+\\]\", \"\", md_text)\n",
    "    md_text = re.sub(r\"\\[\\s*\\]\\([^)]+\\)\", \"\", md_text)\n",
    "    md_text = re.sub(r\"^Wikimedia Commons has media related to .*?\\.\\s*$\", \"\", md_text, flags=re.MULTILINE)\n",
    "    md_text = _drop_tail(md_text)\n",
    "    return _squeeze_blank(md_text)\n",
    "\n",
    "def extract_main(html_text: str) -> tuple[str, str, str]:\n",
    "    soup = BeautifulSoup(html_text, \"lxml\")\n",
    "    title = soup.title.get_text(strip=True) if soup.title else \"Wikipedia Page\"\n",
    "    canonical = soup.find(\"link\", rel=\"canonical\")\n",
    "    source_url = canonical[\"href\"].strip() if canonical and canonical.get(\"href\") else \"\"\n",
    "    main = soup.select_one(\"#mw-content-text\") or soup.select_one(\"main#content\") or soup.select_one(\"#content\")\n",
    "    if not main:\n",
    "        main = soup.body or soup\n",
    "    for sel in REMOVE_SELECTORS:\n",
    "        for tag in main.select(sel):\n",
    "            tag.decompose()\n",
    "    for tag in main.select(\"span.mw-editsection\"):\n",
    "        tag.decompose()\n",
    "    for tag in main.select(\".mw-cite-backlink\"):\n",
    "        tag.decompose()\n",
    "    for tag in main.select(\"table.metadata, table.ambox\"):\n",
    "        tag.decompose()\n",
    "    for a in main.select(\"a[href]\"):\n",
    "        href = a.get(\"href\", \"\")\n",
    "        if href.startswith(\"/wiki/\"):\n",
    "            a[\"href\"] = \"https://en.wikipedia.org\" + href\n",
    "        elif href.startswith(\"//\"):\n",
    "            a[\"href\"] = \"https:\" + href\n",
    "    return title, source_url, str(main)\n",
    "\n",
    "def html_to_markdown(html_text: str) -> tuple[str, str, str]:\n",
    "    title, source_url, main_html = extract_main(html_text)\n",
    "    md_text = md(main_html, heading_style=\"ATX\", bullets=\"-\")\n",
    "    md_text = _post_clean(md_text)\n",
    "    return title, source_url, md_text\n",
    "\n",
    "html_files = sorted(list(INPUT_DIR.glob(\"*.htm\")) + list(INPUT_DIR.glob(\"*.html\")))\n",
    "print(f\"Found {len(html_files)} HTML files in {INPUT_DIR}\")\n",
    "\n",
    "now_iso = datetime.now(timezone.utc).isoformat()\n",
    "converted = 0\n",
    "for fp in html_files:\n",
    "    html_text = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    title, source_url, md_text = html_to_markdown(html_text)\n",
    "    front = [\n",
    "        \"---\",\n",
    "        f\"source_url: {source_url}\",\n",
    "        f\"scraped_at: {now_iso}\",\n",
    "        f\"title: {title}\",\n",
    "        \"description: \",\n",
    "        \"---\",\n",
    "        \"\",\n",
    "    ]\n",
    "    out_path = OUTPUT_DIR / f\"{fp.stem}.md\"\n",
    "    out_path.write_text(\"\\n\".join(front) + md_text + \"\\n\", encoding=\"utf-8\")\n",
    "    converted += 1\n",
    "\n",
    "print(f\"Converted {converted} files -> {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adnlp_hw2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
